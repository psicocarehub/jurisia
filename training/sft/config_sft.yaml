# SFT (Supervised Fine-Tuning) - GAIA Legal Reasoning
# Hyperparameters para treinamento com Unsloth + LoRA

model_name: "CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it"
max_seq_length: 8192
load_in_4bit: true

# LoRA
lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Dataset
dataset_path: "training/data/sft_dataset/train.jsonl"
output_dir: "training/sft/checkpoints"
save_dir: "training/sft/gaia-legal-sft"
gguf_dir: "training/sft/gaia-legal-sft-gguf"

# Training
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 4.0e-4
num_train_epochs: 2
warmup_steps: 50
logging_steps: 10
save_steps: 200
weight_decay: 0.01
lr_scheduler_type: "cosine"
optim: "adamw_8bit"
seed: 42
